{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2002e0e-74c0-4caf-9db1-5926285719c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'documents.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ========== 1. Load the CSV ==========\u001b[39;00m\n\u001b[1;32m     18\u001b[0m CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_PATH)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# assume first column = document name, second column = document content\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'documents.csv'"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF retrieval system (improved version)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Uncomment the next two lines if you haven't downloaded stopwords yet\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"documents.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# assume first column = document name, second column = document content\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "    # candidate docs must contain at least one query term\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = 0.0\n",
    "        for t, qw in q_vec.items():\n",
    "            dw = d_vec.get(t)\n",
    "            if dw is not None:\n",
    "                dot += qw * dw\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Quick demo ==========\n",
    "# Try searching your dataset\n",
    "print(search(\"sample query\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64b0247-d994-4f7c-b7c6-246341ef25cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/chirath/Desktop/documents.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ========== 1. Load the CSV ==========\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 🔥 Replace <your-username> with your Mac username (e.g. chirath)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/chirath/Desktop/documents.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_PATH)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# assume first column = document name, second column = document content\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/chirath/Desktop/documents.csv'"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF retrieval system (improved version, with Desktop path)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Uncomment the next two lines if you haven't downloaded stopwords yet\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "# 🔥 Replace <your-username> with your Mac username (e.g. chirath)\n",
    "CSV_PATH = \"/Users/chirath/Desktop/documents.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# assume first column = document name, second column = document content\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "    # candidate docs must contain at least one query term\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = 0.0\n",
    "        for t, qw in q_vec.items():\n",
    "            dw = d_vec.get(t)\n",
    "            if dw is not None:\n",
    "                dot += qw * dw\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Quick demo ==========\n",
    "print(search(\"sample query\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39bdf93-7eee-4758-88d4-675de823c5f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/chirathwijeweera/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/chirathwijeweera/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ========== 2. Tokenizer ==========\u001b[39;00m\n\u001b[1;32m     34\u001b[0m token_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[a-z]+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     36\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, do_stem: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/chirathwijeweera/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF Retrieval System (Mac Desktop Version)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Uncomment if stopwords or punkt are not downloaded\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"/Users/chirathwijeweera/Desktop/documents.csv\"\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = sum(q_vec[t] * d_vec.get(t, 0.0) for t in q_vec)\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Demo ==========\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your search query: \")\n",
    "    df_results = search(query)\n",
    "    print(\"\\nTop matching documents:\\n\", df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757d0342-98b6-4e4b-8c54-10b973c6b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chirathwijeweera/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chirathwijeweera/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK punkt tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV file not found at /Users/chirath/Desktop/documents.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/chirath/Desktop/documents.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(CSV_PATH):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_PATH)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CSV file not found at /Users/chirath/Desktop/documents.csv"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF Retrieval System (Mac Desktop Version)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# ========== 0. Ensure NLTK resources are downloaded ==========\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"/Users/chirath/Desktop/documents.csv\"\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = sum(q_vec[t] * d_vec.get(t, 0.0) for t in q_vec)\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Demo ==========\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your search query: \")\n",
    "    df_results = search(query)\n",
    "    print(\"\\nTop matching documents:\\n\", df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bad041-8178-436c-85e7-b654c8e328e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching documents found.\n",
      "\n",
      "Top matching documents:\n",
      " Empty DataFrame\n",
      "Columns: [Document, Score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF Retrieval System (Mac Desktop Version)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# ========== 0. Ensure NLTK resources are downloaded ==========\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"/Users/chirathwijeweera/Desktop/documents.csv\"\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = sum(q_vec[t] * d_vec.get(t, 0.0) for t in q_vec)\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Demo ==========\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your search query: \")\n",
    "    df_results = search(query)\n",
    "    print(\"\\nTop matching documents:\\n\", df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3101984d-e6ea-4fb5-bce5-ea133d395ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching documents:\n",
      "                   Document     Score\n",
      "0  The Story Behind Banksy  0.153318\n"
     ]
    }
   ],
   "source": [
    "# Simple TF-IDF Retrieval System (Offline, Mac Desktop Version)\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 0. Built-in stopwords list ==========\n",
    "stop_words = set(\"\"\"\n",
    "a about above after again against all am an and any are aren't as at be because been \n",
    "before being below between both but by can't cannot could couldn't did didn't do does \n",
    "doesn't doing don't down during each few for from further had hadn't has hasn't have \n",
    "haven't having he he'd he'll he's her here here's hers herself him himself his how \n",
    "how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most \n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out \n",
    "over own same shan't she she'd she'll she's should shouldn't so some such than that \n",
    "that's the their theirs them themselves then there there's these they they'd they'll \n",
    "they're they've this those through to too under until up very was wasn't we we'd we'll \n",
    "we're we've were weren't what what's when when's where where's which while who who's \n",
    "whom why why's with won't would wouldn't you you'd you'll you're you've your yours \n",
    "yourself yourselves\n",
    "\"\"\".split())\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"/Users/chirathwijeweera/Desktop/documents.csv\"\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = sum(q_vec[t] * d_vec.get(t, 0.0) for t in q_vec)\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function (returns DataFrame) ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Demo ==========\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your search query: \")\n",
    "    df_results = search(query)\n",
    "    print(\"\\nTop matching documents:\\n\", df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc4d58d5-6850-4308-b487-72a54595a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  machine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching documents found.\n",
      "\n",
      "Top matching documents:\n",
      " Empty DataFrame\n",
      "Columns: [Document, Score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# Offline TF-IDF Document Retrieval System\n",
    "# Author: Your Name\n",
    "# ================================================\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ========== 0. Built-in English stopwords ==========\n",
    "stop_words = set(\"\"\"\n",
    "a about above after again against all am an and any are aren't as at be because been \n",
    "before being below between both but by can't cannot could couldn't did didn't do does \n",
    "doesn't doing don't down during each few for from further had hadn't has hasn't have \n",
    "haven't having he he'd he'll he's her here here's hers herself him himself his how \n",
    "how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most \n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out \n",
    "over own same shan't she she'd she'll she's should shouldn't so some such than that \n",
    "that's the their theirs them themselves then there there's these they they'd they'll \n",
    "they're they've this those through to too under until up very was wasn't we we'd we'll \n",
    "we're we've were weren't what what's when when's where where's which while who who's \n",
    "whom why why's with won't would wouldn't you you'd you'll you're you've your yours \n",
    "yourself yourselves\n",
    "\"\"\".split())\n",
    "\n",
    "# ========== 1. Load the CSV ==========\n",
    "CSV_PATH = \"/Users/chirathwijeweera/Desktop/documents.csv\"\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if df.shape[1] < 2:\n",
    "    raise ValueError(\"CSV must have at least two columns: name, content\")\n",
    "\n",
    "doc_names = df.iloc[:, 0].astype(str).tolist()\n",
    "doc_texts = df.iloc[:, 1].astype(str).tolist()\n",
    "N = len(doc_texts)\n",
    "\n",
    "# ========== 2. Tokenizer ==========\n",
    "token_pattern = re.compile(r\"[a-z]+\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text: str, do_stem: bool = True) -> List[str]:\n",
    "    tokens = [t for t in token_pattern.findall(text.lower()) if t not in stop_words]\n",
    "    if do_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# ========== 3. Build postings & raw counts ==========\n",
    "postings: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_term_counts: List[Counter] = []\n",
    "\n",
    "for doc_id, text in enumerate(doc_texts):\n",
    "    cnt = Counter(tokenize(text))\n",
    "    doc_term_counts.append(cnt)\n",
    "    for term in cnt:\n",
    "        postings[term].append(doc_id)\n",
    "\n",
    "# ========== 4. Document frequencies & IDF ==========\n",
    "dfreq: Dict[str, int] = {term: len(docs) for term, docs in postings.items()}\n",
    "\n",
    "def idf(term: str) -> float:\n",
    "    df_t = dfreq.get(term, 0)\n",
    "    return math.log10(N / df_t) if df_t > 0 else 0.0\n",
    "\n",
    "idf_index: Dict[str, float] = {term: idf(term) for term in postings.keys()}\n",
    "\n",
    "# ========== 5. TF with log scaling + TF-IDF ==========\n",
    "def tf(freq: int) -> float:\n",
    "    return 1.0 + math.log10(freq) if freq > 0 else 0.0\n",
    "\n",
    "doc_tfidf: List[Dict[str, float]] = []\n",
    "doc_norms: List[float] = []\n",
    "\n",
    "for cnt in doc_term_counts:\n",
    "    vec: Dict[str, float] = {}\n",
    "    for term, f in cnt.items():\n",
    "        w = tf(f) * idf_index.get(term, 0.0)\n",
    "        if w != 0.0:\n",
    "            vec[term] = w\n",
    "    norm = math.sqrt(sum(w*w for w in vec.values()))\n",
    "    doc_tfidf.append(vec)\n",
    "    doc_norms.append(norm)\n",
    "\n",
    "# ========== 6. Query vector ==========\n",
    "def build_query_vector(query: str) -> Tuple[Dict[str, float], float, List[str]]:\n",
    "    q_counts = Counter(tokenize(query))\n",
    "    q_vec: Dict[str, float] = {}\n",
    "    used_terms: List[str] = []\n",
    "    for term, f in q_counts.items():\n",
    "        if term in idf_index:\n",
    "            w = tf(f) * idf_index[term]\n",
    "            if w != 0.0:\n",
    "                q_vec[term] = w\n",
    "                used_terms.append(term)\n",
    "    q_norm = math.sqrt(sum(w*w for w in q_vec.values()))\n",
    "    return q_vec, q_norm, used_terms\n",
    "\n",
    "# ========== 7. Scoring with cosine similarity ==========\n",
    "def score_documents(query: str, top_k: int = 5):\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    q_vec, q_norm, q_terms = build_query_vector(query)\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    candidate_docs = set()\n",
    "    for t in q_terms:\n",
    "        candidate_docs.update(postings.get(t, []))\n",
    "\n",
    "    results = []\n",
    "    for doc_id in candidate_docs:\n",
    "        d_vec = doc_tfidf[doc_id]\n",
    "        d_norm = doc_norms[doc_id]\n",
    "        if d_norm == 0.0:\n",
    "            continue\n",
    "        dot = sum(q_vec[t] * d_vec.get(t, 0.0) for t in q_vec)\n",
    "        if dot > 0.0:\n",
    "            sim = dot / (q_norm * d_norm)\n",
    "            results.append((doc_id, sim))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# ========== 8. Search function ==========\n",
    "def search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    results = score_documents(query, top_k=top_k)\n",
    "    if not results:\n",
    "        print(\"No matching documents found.\")\n",
    "        return pd.DataFrame(columns=[\"Document\", \"Score\"])\n",
    "    return pd.DataFrame(\n",
    "        [(doc_names[doc_id], score) for doc_id, score in results],\n",
    "        columns=[\"Document\", \"Score\"]\n",
    "    )\n",
    "\n",
    "# ========== 9. Demo ==========\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your search query: \")\n",
    "    df_results = search(query)\n",
    "    print(\"\\nTop matching documents:\\n\", df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95957a04-caa3-400d-a005-971f0db1a401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
